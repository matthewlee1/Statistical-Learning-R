---
title: "Homework 3"
author: "Hyeonjin Lee"
date: "3/14/2021"
output: pdf_document
---

Introduction to Statistical Learning in R Homework 3 (Chapter 4):  
------------------------------------------------------



### Question 4
> When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that
perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that curse of dinon-parametric approaches often perform poorly when p is large. We mensionality
will now investigate this curse.

a)  
> Suppose that we have a set of observations, each with measurements on p = 1 feature, X. We assume that X is uniformly
(evenly) distributed on [0, 1]. Associated with each observation
is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10 % of
the range of X closest to that test observation. For instance, in
order to predict the response for a test observation with X = 0.6,
we will use observations in the range [0.55, 0.65]. On average,
what fraction of the available observations will we use to make
the prediction?  

> We can use the following expression to find the average fraction of available observations.

$$\int_{.05}^{.95} 10dx +\int_{.0}^{.05} (100x + 5)dx+ \int_{..95}^{1} (105-100x)dx$$
> which equals: 9 + .375 + .375 = 9.75%.  

b)  
> Now suppose that we have a set of observations, each with
measurements on p = 2 features, X1 and X2. We assume that
(X1, X2) are uniformly distributed on [0, 1] × [0, 1]. We wish to
predict a test observation’s response using only observations that
are within 10 % of the range of X1 and within 10 % of the range
of X2 closest to that test observation. For instance, in order to
predict the response for a test observation with X1 = 0.6 and
X2 = 0.35, we will use observations in the range [0.55, 0.65] for
X1 and in the range [0.3, 0.4] for X2. On average, what fraction
of the available observations will we use to make the prediction?  

> We can assume X1 and X2 to be independent. Therefore, the fraction of available observations we use to make the prediction is simply 9.75% * 9.75% or .951%.  

c)  
> Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each
feature, and again each feature ranges in value from 0 to 1. We
wish to predict a test observation’s response using observations
within the 10 % of each feature’s range that is closest to that test
observation. What fraction of the available observations will we
use to make the prediction?  

> Now, the fraction of the available observations will be 9.75% ^ 100 which is very close to 0%.  

d)  
> Using your answers to parts (a)–(c), argue that a drawback of
KNN when p is large is that there are very few training observations “near” any given test observation

> We can see from parts a - c that the fraction of observations is 9.75% to p power. Therefore, when p approaches infinity, the percentage approaches zero.  

e)  
> Now suppose that we wish to make a prediction for a test observation by creating a p-dimensional hypercube centered around
the test observation that contains, on average, 10 % of the training observations. For p = 1, 2, and 100, what is the length of
each side of the hypercube? Comment on your answer.

>Note: A hypercube is a generalization of a cube to an arbitrary
number of dimensions. When p = 1, a hypercube is simply a line
segment, when p = 2 it is a square, and when p = 100 it is a
100-dimensional cube

> We can denote l to be $l = .1^{(1/p)}$  
> p=1, l = .1  
> p = 2, l = $.1^{1/2}$  
> p = 100, l = $.1^{1/100}$  

### Question 9 
> This problem has to do with odds.  

a)
> On average, what fraction of people with an odds of 0.37 of
defaulting on their credit card payment will in fact default? 

> Odds of an event is the probability of y occurring divided by the probability of y not occurring (1-y). This can be written as the following:  

$$\frac{p(y)}{1-p(y)} = .37$$ 
> We can rearrange the formula to find p(y) as:  

$$p(y) = .37(1-p(y)) = .37/1.37 = .27$$  
> Therefore, on average, 27% of people default on their credit card payment.  

b)
>  Suppose that an individual has a 16 % chance of defaulting on
her credit card payment. What are the odds that she will default?  

> Using the same formula, we can now calculate the odds as the following: 

$$\frac{p(y)}{1-p(y)} = \frac{.16}{1-.16} = .19$$  
> The odds that this individual will default is .19%  


### Question 10  
> This question should be answered using the Weekly data set, which
is part of the ISLR package. This data is similar in nature to the
Smarket data from this chapter’s lab, except that it contains 1, 089
weekly returns for 21 years, from the beginning of 1990 to the end of
2010.  

a)  
> Produce some numerical and graphical summaries of the Weekly
data. Do there appear to be any patterns?  

```{r}
library(ISLR)
summary(Weekly)
cor(Weekly[,-9])
plot(Weekly$Year,Weekly$Volume)
```
> We can see from the correlation matrix that the only notable association is between Year and Volume. Looking at the graph of Year vs Volume, we can see that as the years increase, there is also an increase in volume.  

b)  
> Use the full data set to perform a logistic regression with
Direction as the response and the five lag variables plus Volume
as predictors. Use the summary function to print the results. Do
any of the predictors appear to be statistically significant? If so,
which ones?  

```{r}
fit1 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data= Weekly, family = binomial)
summary(fit1)
```
> Only Lag2 is statistically significant is the p-value is less than our alpha of .05.  

c)  
> Compute the confusion matrix and overall fraction of correct
predictions. Explain what the confusion matrix is telling you
about the types of mistakes made by logistic regression.  

```{r}
glm_probs <- predict(fit1, type = "response")
glm_pred <- rep("Down", length(glm_probs))
glm_pred[glm_probs > .5] <- "Up"
table(glm_pred, Weekly$Direction)
```
> From the matrix, we can see that the percentage of correct predictions on the training data is (54+557)/(1089) or 56.107%. Therefore, 43.89% is the training error rate. We can also see that for the weeks in which the market went up, our model is correct (557/(557+48)) or 92.07% of the time. For the weeks in which the market went down,the model is correct (54/54+430) or 11.16% of the time.  

d)  

>  Now fit the logistic regression model using a training data period
from 1990 to 2008, with Lag2 as the only predictor. Compute the
confusion matrix and the overall fraction of correct predictions
for the held out data (that is, the data from 2009 and 2010).  

```{r}
train <- (Weekly$Year < 2009)
Weekly_910 <- Weekly[!train, ]
Direction_910 <- Weekly$Direction[!train]
fit2 <- glm(Direction ~ Lag2, data= Weekly, family = binomial, subset = train)
summary(fit2)

glm_probs2 <- predict(fit2, Weekly_910, type = "response")
glm_pred2 <- rep("Down", length(glm_probs2))
glm_pred2[glm_probs2 > .5] <- "Up"
table(glm_pred2, Direction_910)
```

> From the confusion matrix, we can see that the percentage of correct redictions is (9+56)/104 or 62.5%. Therefore 37.5% is the test error rate. When the market went up, our model was correct (56/56+5) or 91.8% of the time. When the market went down, our model was correct only (9/9+34) or 20.93% of the time.  

e)  
>  Repeat (d) using LDA

```{r}
library(MASS)
lda_fit <- lda(Direction ~ Lag2, data = Weekly, subset = train)
lda_fit

lda_pred <- predict(lda_fit, Weekly_910)
table(lda_pred$class, Direction_910)
```

> From the matrix, we can see that the percentage of correct predictions is 62.5% and thus the test error rate is 37.5% (using the same intuition from previous problems). We can also say that when the market goes up, our model is correct 91.8% of the time and when the market goes down, our model is correct 20.93% of the time. The results are close to the results from the logistic regression.  

f)  
>  Repeat (d) using QDA

```{r}
qda_fit <- qda(Direction ~ Lag2, data = Weekly, subset = train)
qda_fit

qda_pred <- predict(qda_fit, Weekly_910)
table(qda_pred$class, Direction_910)
```
> We can see from the matrix that the percentage of correct predictions is 58.65%. Therefore, 411.35% is the test error rate. For the weeks that the market goes up, our model is correct 100% of the time. For the weeks that the market goes down, our model is correct 0% of the time. However, we should note that our model chooses up the entier time and still receives a correctness of 58.65%.  

g)  
> Repeat (d) using KNN with K = 1.

```{r}
library(class)
train_x <- as.matrix(Weekly$Lag2[train])
test_x <- as.matrix(Weekly$Lag2[!train])
train_Direction <- Weekly$Direction[train]
set.seed(1)
knn_pred <- knn(train_x, test_x, train_Direction, k =1)
table(knn_pred, Direction_910)
```
> From the matrix, we can see that the percentage of correct predictions is 50%. Therefore, the test error rate is also 50%. When the market goes up, the model is correct 50.82% of the time. When the market goes down, our model is correct 48.84% of the time.  

h)  
>  Which of these methods appears to provide the best results on
this data?  

> We can compare the test error rates of the models. We can see that logistic regression and LDA have the lowest error rates. Then it is QDA and KNN.  

i)  
> Experiment with different combinations of predictors, including possible transformations and interactions, for each of the
methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held
out data. Note that you should also experiment with values for
K in the KNN classifier.  

```{r}
# We can try the with Lag1: Lag2

fit3 <- glm(Direction ~ Lag1:Lag2, data = Weekly, family = binomial, subset = train)
glm_probs3 <- predict(fit3, Weekly_910, type ="response")
glm_pred3 <- rep("Down", length(glm_probs3))
glm_pred3[glm_probs3 > .5] = "Up"
table(glm_pred3, Direction_910)

mean(glm_pred3 == Direction_910)

```
```{r}
# LDA with Lag1:Lag2

lda_fit2 <- lda(Direction ~ Lag1:Lag2, data = Weekly, subset = train)
lda_pred2 <- predict(lda_fit2, Weekly_910)
table(lda_pred2$class, Direction_910)
mean(lda_pred2$class == Direction_910)
```
```{r}
# QDA with Lag1:Lag2

qda_fit2 <- qda(Direction ~ Lag1:Lag2, data = Weekly, subset = train)
qda_pred2 <- predict(qda_fit2, Weekly_910)
table(qda_pred2$class, Direction_910)
mean(qda_pred2$class == Direction_910)
```
```{r}
# Knn k = 5

knn_pred2 <- knn(train_x, test_x, train_Direction, k = 5)
table(knn_pred2, Direction_910)
mean(knn_pred2 == Direction_910)

# Knn k = 50
knn_pred3 <- knn(train_x, test_x, train_Direction, k = 50)
table(knn_pred3, Direction_910)
mean(knn_pred3 == Direction_910)
```

> Looking at all the models, we can see that the best performance came from the logistic regression and the LDA models when comparing test rate failures.  